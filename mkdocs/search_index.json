{
    "docs": [
        {
            "location": "/", 
            "text": "SERPS\n\n\nThe PHP Search Engine Result Page Spider\n\n\n\n\n\n\nSERPS is a scraping library for php. It considerably decrease the complexity required to analyse search engines.\n\n\n\n\nWeb scraping\n (web harvesting or web data extraction) is a computer software technique of extracting \ninformation from websites. Usually, such software programs simulate human exploration of the World Wide Web \nby either implementing low-level Hypertext Transfer Protocol (HTTP), \nor embedding a fully-fledged web browser, such as Mozilla Firefox.\n\n\nWikipedia\n\n\n\n\nWhat is it?\n\n\nSerps is a set of tools that ease the parsing of \npopular search engines\n (such as google, yahoo or bing).\nIt helps to parse \nSERP\n (Search Engine Result Page) and gives you a standard output of what is parsed.\n\n\nThe problem\n\n\nMost of times search engines don't want you to parse them, and they don't offer a documentation or a standard way \nto extract the results from the SERP and it's hard to write and maintain a scraper.\n\n\nThe solution\n\n\nTo solve this problems we \nanalysed\n how search engines behave and we built the necessary tools to\nwork with them, from the URL generation to the parsing of the results. \nAt the endpoint we offer a \nstandard and documented API\n.\n\n\nGetting Started\n\n\nLooking forward to work with the library? \n\n\n\n\nStart with the \noverview\n.\n\n\nBrowse the available \nsearch engines\n and \nhttp clients\n from the top menu.\n\n\n\n\nSupport \n issue \n\n\nYou have problems to get started with the library or you have general question? We'd like to hear about,\n\njoin us on gitter\n.\n\n\nYou spotted an issue with the library? Please report it on the \n\ngithub issue tracker\n.\n\n\nLicensing\n\n\nThe work is placed under the terms of the \nFair License\n.\n\n\n\n\nUsage of the works is permitted provided that this instrument is retained with the works, \nso that any entity that uses the works is notified of this instrument.\n\n\nDISCLAIMER: THE WORKS ARE WITHOUT WARRANTY.", 
            "title": "Home"
        }, 
        {
            "location": "/#serps", 
            "text": "The PHP Search Engine Result Page Spider    SERPS is a scraping library for php. It considerably decrease the complexity required to analyse search engines.   Web scraping  (web harvesting or web data extraction) is a computer software technique of extracting \ninformation from websites. Usually, such software programs simulate human exploration of the World Wide Web \nby either implementing low-level Hypertext Transfer Protocol (HTTP), \nor embedding a fully-fledged web browser, such as Mozilla Firefox.  Wikipedia", 
            "title": "SERPS"
        }, 
        {
            "location": "/#what-is-it", 
            "text": "Serps is a set of tools that ease the parsing of  popular search engines  (such as google, yahoo or bing).\nIt helps to parse  SERP  (Search Engine Result Page) and gives you a standard output of what is parsed.", 
            "title": "What is it?"
        }, 
        {
            "location": "/#the-problem", 
            "text": "Most of times search engines don't want you to parse them, and they don't offer a documentation or a standard way \nto extract the results from the SERP and it's hard to write and maintain a scraper.", 
            "title": "The problem"
        }, 
        {
            "location": "/#the-solution", 
            "text": "To solve this problems we  analysed  how search engines behave and we built the necessary tools to\nwork with them, from the URL generation to the parsing of the results. \nAt the endpoint we offer a  standard and documented API .", 
            "title": "The solution"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Looking forward to work with the library?    Start with the  overview .  Browse the available  search engines  and  http clients  from the top menu.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#support-issue", 
            "text": "You have problems to get started with the library or you have general question? We'd like to hear about, join us on gitter .  You spotted an issue with the library? Please report it on the  github issue tracker .", 
            "title": "Support &amp; issue "
        }, 
        {
            "location": "/#licensing", 
            "text": "The work is placed under the terms of the  Fair License .   Usage of the works is permitted provided that this instrument is retained with the works, \nso that any entity that uses the works is notified of this instrument.  DISCLAIMER: THE WORKS ARE WITHOUT WARRANTY.", 
            "title": "Licensing"
        }, 
        {
            "location": "/overview/", 
            "text": "Overview\n\n\nThis overview will help you to understand how the library is built and what are its main components.\n\n\n\n\nInstall\n\n\nTo work with SERPS you need two things:\n\n\n\n\nOne or more search engine clients you want to parse\n\n\nA http client\n\n\n\n\nIn addition \nComposer\n is required to manage the necessary dependencies.\n\n\n\n\ncomposer.json example with the \nGoogle client\n and the \nCurl http client\n\n\n\n\n{\n    \nrequire\n: {\n        \nserps/search-engine-google\n: \n*\n,\n        \nserps/http-client-curl\n: \n*\n\n    }\n}\n\n\n\n\nSearch Engine client\n\n\nIn a regular workflow a search engine client allows to:\n\n\n\n\nManipulate an url and generate a request specific to the search engine\n\n\nRetrieve the response from the search engine\n\n\nParse this response to a standard sets of results\n\n\n\n\nEach \nsearch engine\n has its set of specificities and thus each search engine implementation has its own dedicated guide.\n\n\nThese search engines are currently available:\n\n\n\n\nGoogle\n\n\n\n\nHttp Client\n\n\nWorking with search engines involves to work with \nhttp requests\n.\nUsually the \nsearch engine client\n will need a http client to work correctly.\n\n\n\n\nExample with the \ngoogle client\n and the \ncurl http client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n\n\n\nThere are two kinds of http clients: those that return the raw html as returned from the search engine (e.g the curl client)\nand the others that evaluate the javascript and update the DOM before before returning (e.g the phantomJS client)\n\n\nThese http clients are currently available:\n\n\n\n\nRaw clients:\n\n\nCURL\n\n\n\n\n\n\nEvaluating clients\n\n\nphantomJS\n\n\n\n\n\n\n\n\nProxies\n\n\nMost of time search engines don't want you to parse them thus \nthey use to block you with captcha when they think you are a bot\nWhen you deal with a very \nlarge number of requests\n, you will need to send requests\nthrough proxies.\n\n\nThis is a major feature of scraping and we placed proxies at the very heart of the library. \nEach request is proxy aware. \nThis way, with a single client you can use as many proxies as you want.\n\n\n\n\nExample of \nproxy\n usage with the google client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleClient-\nquery($googleUrl, $proxy);\n\n\n\n\nCaptcha\n\n\nEven though you are using proxies and place all the efforts to act like an human, you might encounter the fatal captcha.\n\n\nWhen you get \nblocked\n by a captcha request, it is very important to stop sending request to the search engine and to\nsolve the captcha before you continue. \n\n\nDealing with captcha is not easy, at the current state the library can detect captcha but is not able to solve them\nfor you we are \ncurrently working\n on a captcha solver implementation.\n\n\n\n\nNote\n\n\nCaptcha are proxy specific, when solving a captcha that should be done with the proxy that was initially blocked\n\n\n\n\nCookies\n\n\nSERPS integrates cookie management, that allows to share cookies across many requests.\n\n\nCookie management is usually done at the search engine client level. You still want to know\nhow to manipulate cookies and cookiejars: \n\nsee cookie documentation", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#overview", 
            "text": "This overview will help you to understand how the library is built and what are its main components.", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#install", 
            "text": "To work with SERPS you need two things:   One or more search engine clients you want to parse  A http client   In addition  Composer  is required to manage the necessary dependencies.   composer.json example with the  Google client  and the  Curl http client   {\n     require : {\n         serps/search-engine-google :  * ,\n         serps/http-client-curl :  * \n    }\n}", 
            "title": "Install"
        }, 
        {
            "location": "/overview/#search-engine-client", 
            "text": "In a regular workflow a search engine client allows to:   Manipulate an url and generate a request specific to the search engine  Retrieve the response from the search engine  Parse this response to a standard sets of results   Each  search engine  has its set of specificities and thus each search engine implementation has its own dedicated guide.  These search engines are currently available:   Google", 
            "title": "Search Engine client"
        }, 
        {
            "location": "/overview/#http-client", 
            "text": "Working with search engines involves to work with  http requests .\nUsually the  search engine client  will need a http client to work correctly.   Example with the  google client  and the  curl http client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());  There are two kinds of http clients: those that return the raw html as returned from the search engine (e.g the curl client)\nand the others that evaluate the javascript and update the DOM before before returning (e.g the phantomJS client)  These http clients are currently available:   Raw clients:  CURL    Evaluating clients  phantomJS", 
            "title": "Http Client"
        }, 
        {
            "location": "/overview/#proxies", 
            "text": "Most of time search engines don't want you to parse them thus \nthey use to block you with captcha when they think you are a bot\nWhen you deal with a very  large number of requests , you will need to send requests\nthrough proxies.  This is a major feature of scraping and we placed proxies at the very heart of the library. \nEach request is proxy aware. \nThis way, with a single client you can use as many proxies as you want.   Example of  proxy  usage with the google client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleClient- query($googleUrl, $proxy);", 
            "title": "Proxies"
        }, 
        {
            "location": "/overview/#captcha", 
            "text": "Even though you are using proxies and place all the efforts to act like an human, you might encounter the fatal captcha.  When you get  blocked  by a captcha request, it is very important to stop sending request to the search engine and to\nsolve the captcha before you continue.   Dealing with captcha is not easy, at the current state the library can detect captcha but is not able to solve them\nfor you we are  currently working  on a captcha solver implementation.   Note  Captcha are proxy specific, when solving a captcha that should be done with the proxy that was initially blocked", 
            "title": "Captcha"
        }, 
        {
            "location": "/overview/#cookies", 
            "text": "SERPS integrates cookie management, that allows to share cookies across many requests.  Cookie management is usually done at the search engine client level. You still want to know\nhow to manipulate cookies and cookiejars:  see cookie documentation", 
            "title": "Cookies"
        }, 
        {
            "location": "/cookies/", 
            "text": "Cookies\n\n\nGuide for cookie and cookiejar manipulation \n\n\n\n\nSERPS offers convenient tools that emulate a cookiejar and allow to persist cookies across many requests.\n\n\nThe following examples introduce the work with cookies\n\n\nCreate a cookie\n\n\nParameters for creating a cookie:\n\n\n\n\nname\n: the name of a cookie.\n\n\nvalue\n: the value of a cookie.\n\n\nflags\n: cookie flags. Available flags are:\n\n\npath\n\n\ndomain\n\n\nexpires\n\n\ndiscard\n\n\nsecure\n\n\n\n\n\n\n\n\nuse Serps\\Core\\Cookie\\Cookie;\n\n$cookie = new Cookie('baz', 'bar', [\n    'domain' =\n 'foo.bar',\n    'path' =\n '/',\n    'expires' =\n time() + 1000,\n]);\n\n\n\n\nPopulate a Cookie Jar\n\n\nuse Serps\\Core\\Cookie\\ArrayCookieJar;\n\n$cookieJar = new ArrayCookieJar();\n\n// Add the cookie 'foo' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('foo', 'bar', ['domain' =\n 'foo.bar']);\n$cookieJar-\nset($cookie);\n\n// Add the cookie 'baz' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('baz', 'bar', ['domain' =\n 'foo.bar']);\n$cookieJar-\nset($cookie);\n\n\n\n\nRetrieves cookies\n\n\nThe \nall\n method is responsible for getting cookies matching some filters\n\n\nMethod parameters:\n\n\n\n\ndomain\n: filters the cookies matching the given domain. Pass null to match all domains.\n\n\npath\n: filters the cookies matching the given path. Pass null to match all paths.\n\n\nname\n: filters the cookies matching the given name. Pass null to match all names.\n\n\nskipDiscardable\n: Set to TRUE to skip cookies with the Discard attribute. Default FALSE.\n\n\nskipExpired\n: Set to FALSE to include expired. Default TRUE.\n\n\n\n\n// Retrieves all cookies\n$cookies = $cookieJar-\nall();\n\n// Retrieves all cookies matching the domain \nfoo.bar\n\n$cookies = $cookieJar-\nall(\nfoo.bar\n);\n\n\n// Retrieves the cookie named \nfoo\n for the domain \nfoo.bar\n, including expired cookies\n$cookies = $cookieJar-\nall(\nfoo.bar\n, \n/\n, \nfoo\n, false, false);\n\n\n\n\nRetrieve cookie for a request\n\n\nIt's possible to automatically retrieve cookies that match a given PSR-7 request:\n\n\n// retrieves all cookies matching the request\n$cookies = $cookieJar-\ngetMatchingCookies($request);\n\n\n\n\nRemove cookies\n\n\n// Remove all cookies\n$cookies = $cookieJar-\nremove();\n\n// Remove all cookies matching the domain \nfoo.bar\n\n$cookies = $cookieJar-\nremove(\nfoo.bar\n);\n\n// Remove the cookie named \nfoo\n for the domain \nfoo.bar\n\n$cookies = $cookieJar-\nremove(\nfoo.bar\n, \n/\n, \nfoo\n);\n\n\n\n\nRemove temporary cookies\n\n\n// Remove all temporary cookies\n$cookies = $cookieJar-\nremoveTemporary();\n\n\n\n\nRemove expired cookies\n\n\n// Remove all cookies that are expired\n$cookies = $cookieJar-\nremoveExpired();", 
            "title": "Cookies"
        }, 
        {
            "location": "/cookies/#cookies", 
            "text": "Guide for cookie and cookiejar manipulation    SERPS offers convenient tools that emulate a cookiejar and allow to persist cookies across many requests.  The following examples introduce the work with cookies", 
            "title": "Cookies"
        }, 
        {
            "location": "/cookies/#create-a-cookie", 
            "text": "Parameters for creating a cookie:   name : the name of a cookie.  value : the value of a cookie.  flags : cookie flags. Available flags are:  path  domain  expires  discard  secure     use Serps\\Core\\Cookie\\Cookie;\n\n$cookie = new Cookie('baz', 'bar', [\n    'domain' =  'foo.bar',\n    'path' =  '/',\n    'expires' =  time() + 1000,\n]);", 
            "title": "Create a cookie"
        }, 
        {
            "location": "/cookies/#populate-a-cookie-jar", 
            "text": "use Serps\\Core\\Cookie\\ArrayCookieJar;\n\n$cookieJar = new ArrayCookieJar();\n\n// Add the cookie 'foo' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('foo', 'bar', ['domain' =  'foo.bar']);\n$cookieJar- set($cookie);\n\n// Add the cookie 'baz' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('baz', 'bar', ['domain' =  'foo.bar']);\n$cookieJar- set($cookie);", 
            "title": "Populate a Cookie Jar"
        }, 
        {
            "location": "/cookies/#retrieves-cookies", 
            "text": "The  all  method is responsible for getting cookies matching some filters  Method parameters:   domain : filters the cookies matching the given domain. Pass null to match all domains.  path : filters the cookies matching the given path. Pass null to match all paths.  name : filters the cookies matching the given name. Pass null to match all names.  skipDiscardable : Set to TRUE to skip cookies with the Discard attribute. Default FALSE.  skipExpired : Set to FALSE to include expired. Default TRUE.   // Retrieves all cookies\n$cookies = $cookieJar- all();\n\n// Retrieves all cookies matching the domain  foo.bar \n$cookies = $cookieJar- all( foo.bar );\n\n\n// Retrieves the cookie named  foo  for the domain  foo.bar , including expired cookies\n$cookies = $cookieJar- all( foo.bar ,  / ,  foo , false, false);", 
            "title": "Retrieves cookies"
        }, 
        {
            "location": "/cookies/#retrieve-cookie-for-a-request", 
            "text": "It's possible to automatically retrieve cookies that match a given PSR-7 request:  // retrieves all cookies matching the request\n$cookies = $cookieJar- getMatchingCookies($request);", 
            "title": "Retrieve cookie for a request"
        }, 
        {
            "location": "/cookies/#remove-cookies", 
            "text": "// Remove all cookies\n$cookies = $cookieJar- remove();\n\n// Remove all cookies matching the domain  foo.bar \n$cookies = $cookieJar- remove( foo.bar );\n\n// Remove the cookie named  foo  for the domain  foo.bar \n$cookies = $cookieJar- remove( foo.bar ,  / ,  foo );", 
            "title": "Remove cookies"
        }, 
        {
            "location": "/cookies/#remove-temporary-cookies", 
            "text": "// Remove all temporary cookies\n$cookies = $cookieJar- removeTemporary();", 
            "title": "Remove temporary cookies"
        }, 
        {
            "location": "/cookies/#remove-expired-cookies", 
            "text": "// Remove all cookies that are expired\n$cookies = $cookieJar- removeExpired();", 
            "title": "Remove expired cookies"
        }, 
        {
            "location": "/search-engine/google/", 
            "text": "Google Client\n\n\n\n\nEverything about the google client\n\n\n\n\nJump to:\n\n\n\n\nParse a google page\n\n\nParse natural results\n\n\nParse adwords results\n\n\n\n\n\n\n\n\n\n\nInstallation\n\n\nThe google client is available with the package \n\nserps/search-engine-google\n: \n\n\n$ composer require 'serps/search-engine-google'\n\n\nOverview\n\n\nThe google client needs a http client interface to be constructed and an url to be parsed.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    // Create a google client using the curl http client\n    $googleClient = new GoogleClient(new CurlClient());\n\n    // Create the url that will be parsed\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        // Do stuff\n    }\n\n\n\n\nWorking with urls\n\n\nGoogleUrl\n class offers many convenient tools to work with google urls.\n\n\nCreate an url\n\n\nThe url builder has the required tools to build an url from scratch.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n    $googleUrl-\nsetLanguageRestriction('lang_en');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\nlr=lang_en\n\n\n\n\nUrl from a string\n\n\nIt's also possible to parse an an existing google url string to an url object.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = GoogleUrl::fromString('https://google.com/search?q=simpsons');\n    echo $googleUrl-\ngetSearchTerm();\n    // simpsons\n\n\n\n\nAdditionally you can continue to manipulate this url\n\n\n    $googleUrl-\nsetLanguageRestriction('lang_en');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\nlr=lang_en\n\n\n\n\nGoogle domain\n\n\nBy default an url is generated for \ngoogle.com\n but you can choose any domain of your choice:\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl('google.fr');\n    $googleUrl-\nsetSearchTerm('simpsons');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.fr/search?q=simpsons\n\n\n\n\nIt's also possible to modify it latter\n\n\n    $googleUrl-\nsetHost('google.de');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.de/search?q=simpsons\n\n\n\n\nAdd and remove parameters\n\n\nIt's possible to add or remove parameters\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetParam('q', 'simpsons');\n    $googleUrl-\nsetParam('start', 11);\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\nstart=11\n\n    $googleUrl-\nremoveParam('start');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\n\n\n\n\nRaw parameters\n\n\nBy default parameters are encoded for urls. For instance \n\"Homer Simpsons\"\n will become \n\"Homer+Simpsons\"\n\nbut \n\"Homer+Simpsons\"\n will become \n\"Homer%2BSimpson\"\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetParam('q', 'Homer+Simpson');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=Homer%2BSimpson\n\n\n\n\nIt's possible to deal with raw params, this way the param will be passed to the url with no additional encoding. \nThat is achieved by passing true as the third argument of \nsetParam\n.\n\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetParam('q', 'Homer+Simpson', true);\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=Homer+Simpson\n\n\n\n\nMore parameters\n\n\nSome parameters are very common and for some of them we created convenient shortcuts.\n\n\nTODO\n\n\nParsing results\n\n\nThe google client is also responsible for parsing the page and outputting a standard result set.\n\n\nA page can return different kind of results. For convenience the guide for parsing a page has its own page: \nview the guide\n\n\nProxy usage\n\n\nYou can use a proxy at the request time\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Core\\Http\\Proxy;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $google-\nsetSearchTerm('simpsons');\n\n    $proxy = new Proxy('1.1.1.1', 8080);\n\n    $response = $googleClient-\nquery($googleUrl, $proxy);\n\n\n\n\nCookie usage\n\n\nThe google client can persist cookies across several requests. \nBy default it is disabled, to enable it, simply do:\n\n\n    $googleClient-\nenableCookies();\n\n\n\n\nAnd to disable it again:\n\n\n    $googleClient-\ndisableCookies();\n\n\n\n\nYou can also get the current state of the cookieJar:\n\n\n    $googleClient-\ngetCookieJar();\n\n\n\n\nOr set a custom CookieJar\n\n\n    $googleClient-\nsetCookieJar($cookieJar);\n\n\n\n\nView the dedicated \ncookie documentation\n\n\nSolve a Captcha\n\n\nSolving captcha is not implemented at the moment", 
            "title": "Google"
        }, 
        {
            "location": "/search-engine/google/#google-client", 
            "text": "Everything about the google client   Jump to:   Parse a google page  Parse natural results  Parse adwords results", 
            "title": "Google Client"
        }, 
        {
            "location": "/search-engine/google/#installation", 
            "text": "The google client is available with the package  serps/search-engine-google :   $ composer require 'serps/search-engine-google'", 
            "title": "Installation"
        }, 
        {
            "location": "/search-engine/google/#overview", 
            "text": "The google client needs a http client interface to be constructed and an url to be parsed.      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    // Create a google client using the curl http client\n    $googleClient = new GoogleClient(new CurlClient());\n\n    // Create the url that will be parsed\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    $response = $googleClient- query($googleUrl);\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        // Do stuff\n    }", 
            "title": "Overview"
        }, 
        {
            "location": "/search-engine/google/#working-with-urls", 
            "text": "GoogleUrl  class offers many convenient tools to work with google urls.", 
            "title": "Working with urls"
        }, 
        {
            "location": "/search-engine/google/#create-an-url", 
            "text": "The url builder has the required tools to build an url from scratch.      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n    $googleUrl- setLanguageRestriction('lang_en');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons lr=lang_en", 
            "title": "Create an url"
        }, 
        {
            "location": "/search-engine/google/#url-from-a-string", 
            "text": "It's also possible to parse an an existing google url string to an url object.      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = GoogleUrl::fromString('https://google.com/search?q=simpsons');\n    echo $googleUrl- getSearchTerm();\n    // simpsons  Additionally you can continue to manipulate this url      $googleUrl- setLanguageRestriction('lang_en');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons lr=lang_en", 
            "title": "Url from a string"
        }, 
        {
            "location": "/search-engine/google/#google-domain", 
            "text": "By default an url is generated for  google.com  but you can choose any domain of your choice:      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl('google.fr');\n    $googleUrl- setSearchTerm('simpsons');\n    echo $googleUrl- buildUrl();\n    // https://google.fr/search?q=simpsons  It's also possible to modify it latter      $googleUrl- setHost('google.de');\n    echo $googleUrl- buildUrl();\n    // https://google.de/search?q=simpsons", 
            "title": "Google domain"
        }, 
        {
            "location": "/search-engine/google/#add-and-remove-parameters", 
            "text": "It's possible to add or remove parameters      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setParam('q', 'simpsons');\n    $googleUrl- setParam('start', 11);\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons start=11\n\n    $googleUrl- removeParam('start');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons", 
            "title": "Add and remove parameters"
        }, 
        {
            "location": "/search-engine/google/#raw-parameters", 
            "text": "By default parameters are encoded for urls. For instance  \"Homer Simpsons\"  will become  \"Homer+Simpsons\" \nbut  \"Homer+Simpsons\"  will become  \"Homer%2BSimpson\"      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setParam('q', 'Homer+Simpson');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=Homer%2BSimpson  It's possible to deal with raw params, this way the param will be passed to the url with no additional encoding. \nThat is achieved by passing true as the third argument of  setParam .      $googleUrl = new GoogleUrl();\n    $googleUrl- setParam('q', 'Homer+Simpson', true);\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=Homer+Simpson", 
            "title": "Raw parameters"
        }, 
        {
            "location": "/search-engine/google/#more-parameters", 
            "text": "Some parameters are very common and for some of them we created convenient shortcuts.  TODO", 
            "title": "More parameters"
        }, 
        {
            "location": "/search-engine/google/#parsing-results", 
            "text": "The google client is also responsible for parsing the page and outputting a standard result set.  A page can return different kind of results. For convenience the guide for parsing a page has its own page:  view the guide", 
            "title": "Parsing results"
        }, 
        {
            "location": "/search-engine/google/#proxy-usage", 
            "text": "You can use a proxy at the request time      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Core\\Http\\Proxy;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $google- setSearchTerm('simpsons');\n\n    $proxy = new Proxy('1.1.1.1', 8080);\n\n    $response = $googleClient- query($googleUrl, $proxy);", 
            "title": "Proxy usage"
        }, 
        {
            "location": "/search-engine/google/#cookie-usage", 
            "text": "The google client can persist cookies across several requests. \nBy default it is disabled, to enable it, simply do:      $googleClient- enableCookies();  And to disable it again:      $googleClient- disableCookies();  You can also get the current state of the cookieJar:      $googleClient- getCookieJar();  Or set a custom CookieJar      $googleClient- setCookieJar($cookieJar);  View the dedicated  cookie documentation", 
            "title": "Cookie usage"
        }, 
        {
            "location": "/search-engine/google/#solve-a-captcha", 
            "text": "Solving captcha is not implemented at the moment", 
            "title": "Solve a Captcha"
        }, 
        {
            "location": "/http-client/curl/", 
            "text": "Curl HTTP Client\n\n\nCurl adapter for a simple scraping implementation\n\n\n\n\nCurl Adapter will allow to use the built in php CURL extension. This adapter \nfully supports proxies and cookies\n\n\nInstallation\n\n\nThe client is available with the package \n\nserps/http-client-curl\n: \n\n\n$ composer require 'serps/http-client-curl'\n\n\nAdditional requirement\n\n\nThis http client requires you to \ninstall the Curl php extension\n\n\nUsage\n\n\nuse Serps\\HttpClient\\PhantomJsClient;\n\n$client = new CurlClient();", 
            "title": "CURL"
        }, 
        {
            "location": "/http-client/curl/#curl-http-client", 
            "text": "Curl adapter for a simple scraping implementation   Curl Adapter will allow to use the built in php CURL extension. This adapter  fully supports proxies and cookies", 
            "title": "Curl HTTP Client"
        }, 
        {
            "location": "/http-client/curl/#installation", 
            "text": "The client is available with the package  serps/http-client-curl :   $ composer require 'serps/http-client-curl'", 
            "title": "Installation"
        }, 
        {
            "location": "/http-client/curl/#additional-requirement", 
            "text": "This http client requires you to  install the Curl php extension", 
            "title": "Additional requirement"
        }, 
        {
            "location": "/http-client/curl/#usage", 
            "text": "use Serps\\HttpClient\\PhantomJsClient;\n\n$client = new CurlClient();", 
            "title": "Usage"
        }, 
        {
            "location": "/http-client/phantomJS/", 
            "text": "PhantomJS HTTP Client\n\n\n\n\nPhantomJS\n is a webkit implementation that helps to simulate the real browser.\n\n\n\n\nBy using this client you will execute the inner javascript code and make the DOM as real as in the true browser,\nthat can be required for some search engines to work properly.\n\n\n\n\nThis is an evaluating adapter\n\n\nWhen you use phantomJS client and you submit a request from it the resulting DOM \nmight be different of the source code returned by a server because\njavascript is executed before returning the DOM.\n\n\n\n\n\n\nNotice about cookies\n\n\nAt the current state phantomJS adapter does not support internal cookieJar usage.\n\n\n\n\nInstallation\n\n\nThe client is available with the package \n\nserps/http-client-phantomjs\n: \n\n\n$ composer require 'serps/http-client-phantomjs'\n\n\nAdditional requirement\n\n\nPhantomJS\n binaries have to be installed\n to use the client. The process for installing\nit depends on your environment, you will find further guides on the internet.\n\n\n\n\nNote\n\n\nThe package \njakoch/phantomjs-installer\n \ncan help you to manage phantomJS as a dependency of your project.\n\n\n\n\nUsage\n\n\nuse Serps\\HttpClient\\PhantomJsClient;\n\n// The constructor accepts 1 optional parameter that is the path to the phantomjs binaries (default to 'phantomjs')\n$client = new PhantomJsClient();", 
            "title": "PhantomJS"
        }, 
        {
            "location": "/http-client/phantomJS/#phantomjs-http-client", 
            "text": "PhantomJS  is a webkit implementation that helps to simulate the real browser.   By using this client you will execute the inner javascript code and make the DOM as real as in the true browser,\nthat can be required for some search engines to work properly.   This is an evaluating adapter  When you use phantomJS client and you submit a request from it the resulting DOM \nmight be different of the source code returned by a server because\njavascript is executed before returning the DOM.    Notice about cookies  At the current state phantomJS adapter does not support internal cookieJar usage.", 
            "title": "PhantomJS HTTP Client"
        }, 
        {
            "location": "/http-client/phantomJS/#installation", 
            "text": "The client is available with the package  serps/http-client-phantomjs :   $ composer require 'serps/http-client-phantomjs'", 
            "title": "Installation"
        }, 
        {
            "location": "/http-client/phantomJS/#additional-requirement", 
            "text": "PhantomJS  binaries have to be installed  to use the client. The process for installing\nit depends on your environment, you will find further guides on the internet.   Note  The package  jakoch/phantomjs-installer  \ncan help you to manage phantomJS as a dependency of your project.", 
            "title": "Additional requirement"
        }, 
        {
            "location": "/http-client/phantomJS/#usage", 
            "text": "use Serps\\HttpClient\\PhantomJsClient;\n\n// The constructor accepts 1 optional parameter that is the path to the phantomjs binaries (default to 'phantomjs')\n$client = new PhantomJsClient();", 
            "title": "Usage"
        }, 
        {
            "location": "/http-client/spidyJS/", 
            "text": "SpidyJS HTTP Client\n\n\nSpidy\n is a browser built with javascript. \n\n\n\n\nThis adapter allows you to query search engines with spidyJS, it is a javascript headless browser. \n\n\n\n\nWarning\n\n\nThis adapter is still a prototype. Use it with care.\n\n\n\n\nInstallation\n\n\nThe client is available with the package \n\nserps/http-client-spidyjs\n: \n\n\n$ composer require 'serps/http-client-spidyjs'\n\n\nAdditional requirement\n\n\nYou will also need nodejs and npm to install spidy.\n\n\n$ npm install -g spidy@2\n\n\n\n\nIf you use a nodejs version that is before 4.0, you will need to install spidy version 1 instead:\n\n\n$ npm install -g spidy@1\n\n\n\n\nUsage\n\n\nuse Serps\\HttpClient\\SpidyJsClient;\n\n\n// The constructor accepts 1 optional parameter that is the path to the spidyjs binaries (default to 'spidyjs')\n$client = new SpidyJsClient();", 
            "title": "SpidyJS"
        }, 
        {
            "location": "/http-client/spidyJS/#spidyjs-http-client", 
            "text": "Spidy  is a browser built with javascript.    This adapter allows you to query search engines with spidyJS, it is a javascript headless browser.    Warning  This adapter is still a prototype. Use it with care.", 
            "title": "SpidyJS HTTP Client"
        }, 
        {
            "location": "/http-client/spidyJS/#installation", 
            "text": "The client is available with the package  serps/http-client-spidyjs :   $ composer require 'serps/http-client-spidyjs'", 
            "title": "Installation"
        }, 
        {
            "location": "/http-client/spidyJS/#additional-requirement", 
            "text": "You will also need nodejs and npm to install spidy.  $ npm install -g spidy@2  If you use a nodejs version that is before 4.0, you will need to install spidy version 1 instead:  $ npm install -g spidy@1", 
            "title": "Additional requirement"
        }, 
        {
            "location": "/http-client/spidyJS/#usage", 
            "text": "use Serps\\HttpClient\\SpidyJsClient;\n\n\n// The constructor accepts 1 optional parameter that is the path to the spidyjs binaries (default to 'spidyjs')\n$client = new SpidyJsClient();", 
            "title": "Usage"
        }, 
        {
            "location": "/about/packages/", 
            "text": "Packages\n\n\nList of packages that are part of this project\n\n\n\n\nCore\n\n\nThis is the core of SERPS. It contains the common tools that are used by search engine and http client implementations\n\n\n Github\n\n\n$ composer require serps/serps\n\n\n\n\n\n\n\n\n\n\nSearch engines\n\n\nGoogle\n\n\nThe google client implementation\n\n\n Github\n\n\n$ composer require serps/search-engine-google\n\n\n\n\n\n\n\n\n\n\nHttp clients\n\n\nCurl\n\n\nCurl Http client\n\n\n Doc\n\n\n Github\n\n\n$ composer require serps/http-client-curl\n\n\n\n\n\n\n\n\nPhantomJS\n\n\nPhantomJS Http client\n\n\n Doc\n\n\n Github\n\n\n$ composer require serps/http-client-phantomjs\n\n\n\n\n\n\n\n\n\n\nSpidyJS\n\n\nnodejs browser built specially for SERPS\n\n\n Github\n\n\n$ npm install spidyjs\n\n\n\nmaster (node \n= 4):\n\n\n1.x (node \n 4):\n\n\n\nWebsite homepage\n\n\nSimply the homepage at https://serp-spider.github.io/\n\n\n Github\n\n\nDocumentation\n\n\nThe package that contains this documentation at http://serp-spider.github.io/documentation\n\n\n Github\n\n\nStatus monitor\n\n\nCli application that helps to monitor search engine changes\n\n\n Github", 
            "title": "Packages"
        }, 
        {
            "location": "/about/packages/#packages", 
            "text": "List of packages that are part of this project", 
            "title": "Packages"
        }, 
        {
            "location": "/about/packages/#core", 
            "text": "This is the core of SERPS. It contains the common tools that are used by search engine and http client implementations   Github  $ composer require serps/serps", 
            "title": "Core"
        }, 
        {
            "location": "/about/packages/#search-engines", 
            "text": "", 
            "title": "Search engines"
        }, 
        {
            "location": "/about/packages/#google", 
            "text": "The google client implementation   Github  $ composer require serps/search-engine-google", 
            "title": "Google"
        }, 
        {
            "location": "/about/packages/#http-clients", 
            "text": "", 
            "title": "Http clients"
        }, 
        {
            "location": "/about/packages/#curl", 
            "text": "Curl Http client   Doc   Github  $ composer require serps/http-client-curl", 
            "title": "Curl"
        }, 
        {
            "location": "/about/packages/#phantomjs", 
            "text": "PhantomJS Http client   Doc   Github  $ composer require serps/http-client-phantomjs", 
            "title": "PhantomJS"
        }, 
        {
            "location": "/about/packages/#spidyjs", 
            "text": "nodejs browser built specially for SERPS   Github  $ npm install spidyjs  \nmaster (node  = 4): \n1.x (node   4):", 
            "title": "SpidyJS"
        }, 
        {
            "location": "/about/packages/#website-homepage", 
            "text": "Simply the homepage at https://serp-spider.github.io/   Github", 
            "title": "Website homepage"
        }, 
        {
            "location": "/about/packages/#documentation", 
            "text": "The package that contains this documentation at http://serp-spider.github.io/documentation   Github", 
            "title": "Documentation"
        }, 
        {
            "location": "/about/packages/#status-monitor", 
            "text": "Cli application that helps to monitor search engine changes   Github", 
            "title": "Status monitor"
        }, 
        {
            "location": "/search-engine/google/parse-page/", 
            "text": "Parse a Google Page\n\n\n\n\nThe necessary documentation about parsing a google page\n\n\n\n\nBack to the \ngeneral google documentation\n.\n\n\n\n\n\n\nImportant notice about google update\n\n\nThe current documentation can change at any time. As soon as google changes its page structure\nthe following example may stop to work correctly.\n\n\nWe place efforts to monitor the changes but we cannot guarantee that everything will be available at any time.\n\n\nRemember to tune your composer.json correctly to make sure to bring new changes as they come. \nWe use the \nsemantic versioning\n \nand the best practise is to \nuse the tilde operator\n. \n\n\n\n\n\n\nA google SERP can contain different type of result.\nFirstly they are divided in three distinct regions: \nnatural\n (organic), \npaid\n (adwords) and \ngraph results\n and each of them\nhas its own results types. Graph result are currently \nnot supported\n by the library.\n\n\nThrough there is a great diversity of results the library gives you the api to work with them, here we document\nwhat are their differences.\n\n\nNatural Results\n\n\nNatural results (aka organic results) are main results of the page.\n\n\nEach natural result has a position and some available data. You can access them the following way (see the foreach loop):\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleClient = new GoogleClient($httpClient);\n\n    $googleUrl = new GoogleUrl();\n    $google-\nsetSearchTerm('simpsons');\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        // Here we iterate over the result list\n        // Each result will have different data based on its type\n    }\n\n\n\n\nEach of the result from the loop will have the following methods available:\n\n\n\n\ngetTypes()\n: the types of the result\n\n\nis($type)\n: check if the result is of the given type\n\n\ngetDataValue($type)\n: Get the given data from the result\n\n\ngetData()\n: Get the all the data of the result\n\n\ngetOnPagePosition()\n: Get the position of the result on the page (not aware of the pagination)\n\n\ngetRealPosition()\n: Get the global position of the result (aware of the pagination)\n\n\n\n\nThe difference between each result type is the list of data available with \ngetDataValue($type)\n and \ngetData()\n.\nSee bellow for all available data per result type.\n\n\nNatural Result Types\n\n\nResult types\n can be accessed through the class \nNaturalResultType\n,\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    if($result-\nis(NaturalResultType::CLASSICAL)){\n        // Do stuff\n    }\n\n    // You can also check many types at once\n    // Here we check if the result is classical or image group\n\n    if($result-\nis(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP)){\n        // Do stuff\n    }\n\n\n\n\nFrom the \nresultSet\n you can also access all the results matching one of the given type:\n\n\n    // Get all the results that are either classical or image_group\n    $results = $results-\ngetResultsByType(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP);\n\n\n\n\nClassical\n\n\nThese results are the common natural results that have always existed in google.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::CLASSICAL\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n [\nA\n]\n\n\nurl\n \nstring\n: the url targeted on clicking the title\n\n\ndestination\n \nstring\n [\nB\n]: either a url or a breadcrumb-like destination\n\n\ndescription\n \nstring\n [\nC\n]\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::CLASSICAL)){\n            $title = $result-\ngetDataValue('title');\n            $url   = $result-\ngetDataValue('url');\n        }\n    }\n\n\n\n\nClassical Video\n\n\nThis type an extension of the \nclassical result\n, but it refers to a video result.\n\n\nThe video result can be illustrated with either a thumbnail or a large image.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::CLASSICAL_VIDEO\n\n\nNaturalResultType::CLASSICAL\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n [\nA\n]\n\n\nurl\n \nstring\n: the url targeted on clicking the title\n\n\ndestination\n \nstring\n [\nB\n]: either a url or a breadcrumb-like destination\n\n\ndescription\n \nstring\n [\nC\n]\n\n\nvideoLarge\n \nbool\n: true if the video is image is large (usually first result)\n\n\nvideoCover\n \nstring\n: the video picture as given by google - either an image url or a base64 encoded image\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::CLASSICAL_VIDEO)){\n            $title = $result-\ngetDataValue('title');\n            if($result-\ngetDataValue('videoLarge'){\n                // ...\n            }\n        }\n    }\n\n\n\n\nImage Group\n\n\nImages that appear as a group of results.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::IMAGE_GROUP\n\n\n\n\nData\n\n\n\n\nimages\n \narray\n: the list of images that compose the image group, each image contains:\n\n\nsourceUrl\n \nUrl\n: the url where the image was found\n\n\ntargetUrl\nUrl\n: the url reached on clicking the image\n\n\nimage\n \nstring\n: the image data as specified by google (either an image url or a base64 encoded image)\n\n\n\n\n\n\nmoreUrl\n \nUrl\n: The url corresponding to the google image search\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::IMAGE_GROUP)){\n            foreach($result-\ngetDataValue('images') as $image){\n                $sourceUrl = $image-\ngetDataValue('sourceUrl');\n            }\n        }\n    }\n\n\n\n\nMap\n\n\nA result illustrated by a map and that contains sub-results.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::MAP\n\n\n\n\nData\n\n\n\n\nlocalPack\n \narray\n: The sub results for the map:\n\n\ntitle\n \nstring\n \n[A]\n: Name of the place\n\n\nurl\nUrl\n \n[B]\n: Website of the sub-result\n\n\nstreet\n \nstring\n \n[C]\n: The address of the sub-result\n\n\nstars\n \nstring\n \n[D]\n: The rating of the result as a number\n\n\nreview\n \nstring\n \n[E]\n: The review string as specified by google (e.g '1 review')\n\n\nphone\n \nstring\n \n[G]\n: The phone number\n\n\n\n\n\n\nmapUrl\n \nUrl\n \n[F]\n: The url to access the map search\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::MAP)){\n            foreach($result-\ngetDataValue('localPack') as $place){\n                $website = $place-\ngetDataValue('website');\n            }\n        }\n    }\n\n\n\n\nTweet Carousel\n\n\nRecent tweet list from an user matching the search keywords.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::TWEETS_CAROUSEL\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n \n[A]\n\n\nurl\n \nstring\n: The url reach when clicking the title\n\n\nuser\nUrl\n: The author of the tweets\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::TWEETS_CAROUSEL)){\n            $user = $result-\ngetDataValue('user');\n        }\n    }\n\n\n\n\nIn the News\n\n\nRecent news results.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::IN_THE_NEWS\n\n\n\n\nData\n\n\n\n\nnews\n \narray\n\n\ntitle\n \nstring\n \n[A]\n\n\ndescription\n \nUrl\n \n[B]\n\n\nurl\nstring\n: The url reached when clicking the title\n\n\n\n\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::IN_THE_NEWS)){\n            $title = $result-\ngetDataValue('title');\n        }\n    }\n\n\n\n\nAdwords Results\n\n\nThe google client offers an Adwords parser.\n\n\n    $adwordsResults = $response-\ngetAdwordsResults();\n\n    foreach($results as $result){\n        // do stuff\n    }\n\n\n\n\nAdwords sections\n\n\nAdwords results are composed from 3 distinct sections. These sections can be at the top, at the right or at the bottom\nof the natural results. See the schema:\n\n\n\n\nBy default all results are available in the result set, if you need \nto get results from a section, you can use the section as a type filter:\n\n\n    use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $adwordsResults = $response-\ngetAdwordsResults();\n\n    $topResults = $adwordsResults-\ngetResultsByType(AdwordsResultType::SECTION_TOP);\n    $rightResults = $adwordsResults-\ngetResultsByType(AdwordsResultType::SECTION_RIGHT);\n    $bottomResults = $adwordsResults-\ngetResultsByType(AdwordsResultType::SECTION_BOTTOM);\n\n    foreach($topResults as $result){\n        // Do stuff...\n    }\n\n\n\n\nAdwords Types\n\n\nAd\n\n\nAds results are the basics results from adwords.\n\n\n\n\nAvailable with\n\n\n\n\nAdwordsResultType::AD\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n \n[A]\n\n\nurl\n \nurl\n: The url reach when clicking the title\n\n\nvisurl\n \nstring\n \n[B]\n: The visual url\n\n\ndescription\nstring\n \n[C]\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n\n    $results = $response-\ngetAdwordsResults();\n\n    foreach($results as $result){\n        if($result-\nis(AdwordsResultType::AD)){\n            $url = $result-\ngetDataValue('url');\n        }\n    }\n\n\n\n\nShopping\n\n\nThese are the results from google shopping/merchant.\n\n\n\n\nAvailable with\n\n\n\n\nAdwordsResultType::SHOPPING_GROUP\n\n\n\n\nData\n\n\n\n\nproducts\n \narray\n: The product list. Each product contains the following items:\n\n\ntitle\n \nstring\n \n[A]\n\n\nimage\n \nstring\n \n[B]\n\n\nurl\n \nurl\n: The url reached when clicking the title\n\n\ntarget\n \nstring\n \n[C]\n: The target website as shown by google\n\n\nprice\nstring\n \n[D]\n: The price as show by google\n\n\n\n\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $results = $response-\ngetAdwordsResults();\n\n    foreach($results as $result){\n        if($result-\nis(AdwordsResultType::SHOPPING_GROUP)){\n            foreach($result-\ngetDataValue('products') as $item){\n                $title = $item-\ngetDataValue('title');\n            }\n        }\n    }\n\n\n\n\nRelated searches\n\n\nNot implemented yet.\n\n\nCustom parsing\n\n\nSometimes you need information that are not available in our parser. \n\n\nFirst of all, search if someone already asked for this feature \non the \nissue tracker\n. \n\n\nIf you don't find a trace of this feature, but you still consider that this feature is important, then open an issue and\nlet's discuss it. This is very important because if the feature is implemented in the library it will take advantage of\nbeing updated on google updates, and you wont have to maintain it.\n\n\n\n\nBack from the issue tracker, no one mentioned it and you still \nwant to parse the information by yourself\n.\n Alright, here are the tools you need.\n\n\nQuery with css\n\n\nThe easiest way to do it for a web developer: \nwith css\n.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    // Returns \\DOMNodeList\n    $queryResult = $response-\ncssQuery('#someId');\n\n    if ($queryResult-\nlength == 1) {\n        // You can query again to find items in the previous context.\n\n        // Gets all items with the class 'someClass' within the element with the id 'someId'\n        $queryResult = $response-\ncssQuery('.someClass', $queryResult-\nitem(0));\n    } else {\n        // some errors...\n    }\n\n\n\n\nIt works exactly as \nDOMXPath::query\n does. Actually the css is translated \nto xpath and \nDOMXPath::query\n is called on the dom element.\n\n\nQuery with xpath\n\n\nThat's very similar to the css way, except that you will use \nxpath\n.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $queryResult = $response-\ncssQuery('descendant::div[@id=\nsomeId\n]');\n\n    if ($queryResult-\nlength == 1) {\n        // Gets all 'a' tags inside the element with the id 'someId'.\n        $queryResult = $response-\ncssQuery('a', $queryResult-\nitem(0));\n    } else {\n        // some errors...\n    }\n\n\n\n\nThere is also a shortcut to the xpath object.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $xpath = $response-\ngetXpath();\n    $xpath-\nquery('someXpath');\n\n\n\n\nManipulate the DOM object\n\n\nYou can get the \nDOM object\n to manipulate it, or to save it in a file.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $dom = $response-\ngetDom();\n\n    // Writes the dom content in the file 'file.html'\n    $dom-\nsave('file.html');", 
            "title": "Parse a Page"
        }, 
        {
            "location": "/search-engine/google/parse-page/#parse-a-google-page", 
            "text": "The necessary documentation about parsing a google page   Back to the  general google documentation .    Important notice about google update  The current documentation can change at any time. As soon as google changes its page structure\nthe following example may stop to work correctly.  We place efforts to monitor the changes but we cannot guarantee that everything will be available at any time.  Remember to tune your composer.json correctly to make sure to bring new changes as they come. \nWe use the  semantic versioning  \nand the best practise is to  use the tilde operator .     A google SERP can contain different type of result.\nFirstly they are divided in three distinct regions:  natural  (organic),  paid  (adwords) and  graph results  and each of them\nhas its own results types. Graph result are currently  not supported  by the library.  Through there is a great diversity of results the library gives you the api to work with them, here we document\nwhat are their differences.", 
            "title": "Parse a Google Page"
        }, 
        {
            "location": "/search-engine/google/parse-page/#natural-results", 
            "text": "Natural results (aka organic results) are main results of the page.  Each natural result has a position and some available data. You can access them the following way (see the foreach loop):      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleClient = new GoogleClient($httpClient);\n\n    $googleUrl = new GoogleUrl();\n    $google- setSearchTerm('simpsons');\n\n    $response = $googleClient- query($googleUrl);\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        // Here we iterate over the result list\n        // Each result will have different data based on its type\n    }  Each of the result from the loop will have the following methods available:   getTypes() : the types of the result  is($type) : check if the result is of the given type  getDataValue($type) : Get the given data from the result  getData() : Get the all the data of the result  getOnPagePosition() : Get the position of the result on the page (not aware of the pagination)  getRealPosition() : Get the global position of the result (aware of the pagination)   The difference between each result type is the list of data available with  getDataValue($type)  and  getData() .\nSee bellow for all available data per result type.", 
            "title": "Natural Results"
        }, 
        {
            "location": "/search-engine/google/parse-page/#natural-result-types", 
            "text": "Result types  can be accessed through the class  NaturalResultType ,      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    if($result- is(NaturalResultType::CLASSICAL)){\n        // Do stuff\n    }\n\n    // You can also check many types at once\n    // Here we check if the result is classical or image group\n\n    if($result- is(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP)){\n        // Do stuff\n    }  From the  resultSet  you can also access all the results matching one of the given type:      // Get all the results that are either classical or image_group\n    $results = $results- getResultsByType(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP);", 
            "title": "Natural Result Types"
        }, 
        {
            "location": "/search-engine/google/parse-page/#classical", 
            "text": "These results are the common natural results that have always existed in google.   Available with   NaturalResultType::CLASSICAL   Data   title   string  [ A ]  url   string : the url targeted on clicking the title  destination   string  [ B ]: either a url or a breadcrumb-like destination  description   string  [ C ]   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::CLASSICAL)){\n            $title = $result- getDataValue('title');\n            $url   = $result- getDataValue('url');\n        }\n    }", 
            "title": "Classical"
        }, 
        {
            "location": "/search-engine/google/parse-page/#classical-video", 
            "text": "This type an extension of the  classical result , but it refers to a video result.  The video result can be illustrated with either a thumbnail or a large image.   Available with   NaturalResultType::CLASSICAL_VIDEO  NaturalResultType::CLASSICAL   Data   title   string  [ A ]  url   string : the url targeted on clicking the title  destination   string  [ B ]: either a url or a breadcrumb-like destination  description   string  [ C ]  videoLarge   bool : true if the video is image is large (usually first result)  videoCover   string : the video picture as given by google - either an image url or a base64 encoded image   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::CLASSICAL_VIDEO)){\n            $title = $result- getDataValue('title');\n            if($result- getDataValue('videoLarge'){\n                // ...\n            }\n        }\n    }", 
            "title": "Classical Video"
        }, 
        {
            "location": "/search-engine/google/parse-page/#image-group", 
            "text": "Images that appear as a group of results.   Available with   NaturalResultType::IMAGE_GROUP   Data   images   array : the list of images that compose the image group, each image contains:  sourceUrl   Url : the url where the image was found  targetUrl Url : the url reached on clicking the image  image   string : the image data as specified by google (either an image url or a base64 encoded image)    moreUrl   Url : The url corresponding to the google image search   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::IMAGE_GROUP)){\n            foreach($result- getDataValue('images') as $image){\n                $sourceUrl = $image- getDataValue('sourceUrl');\n            }\n        }\n    }", 
            "title": "Image Group"
        }, 
        {
            "location": "/search-engine/google/parse-page/#map", 
            "text": "A result illustrated by a map and that contains sub-results.   Available with   NaturalResultType::MAP   Data   localPack   array : The sub results for the map:  title   string   [A] : Name of the place  url Url   [B] : Website of the sub-result  street   string   [C] : The address of the sub-result  stars   string   [D] : The rating of the result as a number  review   string   [E] : The review string as specified by google (e.g '1 review')  phone   string   [G] : The phone number    mapUrl   Url   [F] : The url to access the map search   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::MAP)){\n            foreach($result- getDataValue('localPack') as $place){\n                $website = $place- getDataValue('website');\n            }\n        }\n    }", 
            "title": "Map"
        }, 
        {
            "location": "/search-engine/google/parse-page/#tweet-carousel", 
            "text": "Recent tweet list from an user matching the search keywords.   Available with   NaturalResultType::TWEETS_CAROUSEL   Data   title   string   [A]  url   string : The url reach when clicking the title  user Url : The author of the tweets   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::TWEETS_CAROUSEL)){\n            $user = $result- getDataValue('user');\n        }\n    }", 
            "title": "Tweet Carousel"
        }, 
        {
            "location": "/search-engine/google/parse-page/#in-the-news", 
            "text": "Recent news results.   Available with   NaturalResultType::IN_THE_NEWS   Data   news   array  title   string   [A]  description   Url   [B]  url string : The url reached when clicking the title     Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::IN_THE_NEWS)){\n            $title = $result- getDataValue('title');\n        }\n    }", 
            "title": "In the News"
        }, 
        {
            "location": "/search-engine/google/parse-page/#adwords-results", 
            "text": "The google client offers an Adwords parser.      $adwordsResults = $response- getAdwordsResults();\n\n    foreach($results as $result){\n        // do stuff\n    }", 
            "title": "Adwords Results"
        }, 
        {
            "location": "/search-engine/google/parse-page/#adwords-sections", 
            "text": "Adwords results are composed from 3 distinct sections. These sections can be at the top, at the right or at the bottom\nof the natural results. See the schema:   By default all results are available in the result set, if you need \nto get results from a section, you can use the section as a type filter:      use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $adwordsResults = $response- getAdwordsResults();\n\n    $topResults = $adwordsResults- getResultsByType(AdwordsResultType::SECTION_TOP);\n    $rightResults = $adwordsResults- getResultsByType(AdwordsResultType::SECTION_RIGHT);\n    $bottomResults = $adwordsResults- getResultsByType(AdwordsResultType::SECTION_BOTTOM);\n\n    foreach($topResults as $result){\n        // Do stuff...\n    }", 
            "title": "Adwords sections"
        }, 
        {
            "location": "/search-engine/google/parse-page/#adwords-types", 
            "text": "", 
            "title": "Adwords Types"
        }, 
        {
            "location": "/search-engine/google/parse-page/#ad", 
            "text": "Ads results are the basics results from adwords.   Available with   AdwordsResultType::AD   Data   title   string   [A]  url   url : The url reach when clicking the title  visurl   string   [B] : The visual url  description string   [C]   Example      use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n\n    $results = $response- getAdwordsResults();\n\n    foreach($results as $result){\n        if($result- is(AdwordsResultType::AD)){\n            $url = $result- getDataValue('url');\n        }\n    }", 
            "title": "Ad"
        }, 
        {
            "location": "/search-engine/google/parse-page/#shopping", 
            "text": "These are the results from google shopping/merchant.   Available with   AdwordsResultType::SHOPPING_GROUP   Data   products   array : The product list. Each product contains the following items:  title   string   [A]  image   string   [B]  url   url : The url reached when clicking the title  target   string   [C] : The target website as shown by google  price string   [D] : The price as show by google     Example      use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $results = $response- getAdwordsResults();\n\n    foreach($results as $result){\n        if($result- is(AdwordsResultType::SHOPPING_GROUP)){\n            foreach($result- getDataValue('products') as $item){\n                $title = $item- getDataValue('title');\n            }\n        }\n    }", 
            "title": "Shopping"
        }, 
        {
            "location": "/search-engine/google/parse-page/#related-searches", 
            "text": "Not implemented yet.", 
            "title": "Related searches"
        }, 
        {
            "location": "/search-engine/google/parse-page/#custom-parsing", 
            "text": "Sometimes you need information that are not available in our parser.   First of all, search if someone already asked for this feature \non the  issue tracker .   If you don't find a trace of this feature, but you still consider that this feature is important, then open an issue and\nlet's discuss it. This is very important because if the feature is implemented in the library it will take advantage of\nbeing updated on google updates, and you wont have to maintain it.   Back from the issue tracker, no one mentioned it and you still  want to parse the information by yourself .\n Alright, here are the tools you need.", 
            "title": "Custom parsing"
        }, 
        {
            "location": "/search-engine/google/parse-page/#query-with-css", 
            "text": "The easiest way to do it for a web developer:  with css .      $response = $googleClient- query($googleUrl);\n\n    // Returns \\DOMNodeList\n    $queryResult = $response- cssQuery('#someId');\n\n    if ($queryResult- length == 1) {\n        // You can query again to find items in the previous context.\n\n        // Gets all items with the class 'someClass' within the element with the id 'someId'\n        $queryResult = $response- cssQuery('.someClass', $queryResult- item(0));\n    } else {\n        // some errors...\n    }  It works exactly as  DOMXPath::query  does. Actually the css is translated \nto xpath and  DOMXPath::query  is called on the dom element.", 
            "title": "Query with css"
        }, 
        {
            "location": "/search-engine/google/parse-page/#query-with-xpath", 
            "text": "That's very similar to the css way, except that you will use  xpath .      $response = $googleClient- query($googleUrl);\n\n    $queryResult = $response- cssQuery('descendant::div[@id= someId ]');\n\n    if ($queryResult- length == 1) {\n        // Gets all 'a' tags inside the element with the id 'someId'.\n        $queryResult = $response- cssQuery('a', $queryResult- item(0));\n    } else {\n        // some errors...\n    }  There is also a shortcut to the xpath object.      $response = $googleClient- query($googleUrl);\n\n    $xpath = $response- getXpath();\n    $xpath- query('someXpath');", 
            "title": "Query with xpath"
        }, 
        {
            "location": "/search-engine/google/parse-page/#manipulate-the-dom-object", 
            "text": "You can get the  DOM object  to manipulate it, or to save it in a file.      $response = $googleClient- query($googleUrl);\n\n    $dom = $response- getDom();\n\n    // Writes the dom content in the file 'file.html'\n    $dom- save('file.html');", 
            "title": "Manipulate the DOM object"
        }
    ]
}